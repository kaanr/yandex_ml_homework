{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feUVUO4f2QnD"
   },
   "source": [
    "## HW 2: Crossentropy method\n",
    "_Reference: based on Practical RL course by YSDA_\n",
    "\n",
    "In this notebook you have to solve two simple RL problems with crossentropy method.\n",
    "\n",
    "To get used to `gymnasium` package, please, refer to the [documentation](https://gymnasium.farama.org/introduction/basic_usage/).\n",
    "\n",
    "\n",
    "In the end of the notebook, please, copy the functions you have implemented to file `template_crossentropy.py` and submit it to the Contest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "42jUgUtZ2QnF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "Gymnasium version: 1.2.0\n",
      "NumPy version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"Gymnasium version: {gym.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7TS0B8B2QnF"
   },
   "source": [
    "## Part 1. Tabular CrossEntropy\n",
    "Let's consider discrete game \"Taxi\".\n",
    "\n",
    "There are four designated pick-up and drop-off locations (Red, Green, Yellow and Blue) in the 5x5 grid world. The taxi starts off at a random square and the passenger at one of the designated locations.\n",
    "\n",
    "The goal is move the taxi to the passenger’s location, pick up the passenger, move to the passenger’s desired destination, and drop off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "\n",
    "The player receives positive rewards for successfully dropping-off the passenger at the correct location. Negative rewards for incorrect attempts to pick-up/drop-off passenger and for each step where another reward is not received.\n",
    "\n",
    "You can find full description of the environment [here](https://gymnasium.farama.org/environments/toy_text/taxi/).\n",
    "\n",
    "![Taxi-v3](https://gymnasium.farama.org/_images/taxi.gif)\n",
    "\n",
    "So here's how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "Pe9pr1qP2QnG",
    "outputId": "5dd3fad0-bd50-4ad0-f570-fd928d330146"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x74e4c7533e10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our taxi game world, the second parameter means\n",
    "# we can see it as an image\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
    "# Starts a fresh episode, randomly places taxi, passenger and destination\n",
    "# Returns the starting state number\n",
    "env.reset()\n",
    "\n",
    "# Take a \"screenshot\" of the game\n",
    "# Shows it as a visual image\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ilaorMTO2QnG"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p4_Pbslk2QnG",
    "outputId": "fa8813ea-cec9-4e05-acd8-0b49f5832c9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_states=500, n_actions=6\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"n_states=%i, n_actions=%i\" % (n_states, n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an action 1 and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state, reward, is_done, _, _ = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got to state 72, the reward is -1, game is not finished because is_done=False\n"
     ]
    }
   ],
   "source": [
    "print(f\"We got to state {next_state}, the reward is {reward}, game is not finished because is_done={is_done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DHEUSwR2QnG"
   },
   "source": [
    "### Create stochastic policy\n",
    "\n",
    "This time our policy should be a probability distribution.\n",
    "\n",
    "```policy[s,a] = P(take action a | in state s)```\n",
    "\n",
    "Since we still use integer state and action representations, you can use a 2-dimensional array to represent the policy.\n",
    "\n",
    "Please initialize policy __uniformly__, that is, probabililities of all actions should be equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7zLIqLRy2QnG"
   },
   "outputs": [],
   "source": [
    "policy = np.ones((n_states, n_actions)) / n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HvnhwGJr2QnG"
   },
   "outputs": [],
   "source": [
    "assert type(policy) in (np.ndarray, np.matrix)\n",
    "assert np.allclose(policy, 1./n_actions)\n",
    "assert np.allclose(np.sum(policy, axis=1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SOYGjlnw2QnH",
    "outputId": "a93cf507-3152-4106-de2f-fab68b52bb3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "        0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "        0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "        0.16666667],\n",
       "       ...,\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "        0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "        0.16666667],\n",
       "       [0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,\n",
       "        0.16666667]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZlYuiQw2QnH"
   },
   "source": [
    "### Play the game\n",
    "\n",
    "Just like before, but we also record all states and actions we took.\n",
    "\n",
    "Sample the action (e.g. `np.random.choice()`) from available actions and make a step in the environment using `env.step()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "motIObIf2QnH"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, policy, t_max=int(10**4)):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    :param policy: an array of shape [n_states,n_actions] with action probabilities\n",
    "    :returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states, actions = [], []\n",
    "    total_reward = 0.\n",
    "\n",
    "    s, info = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # your code here - sample action from policy and get new state, reward, done flag etc. from the environment\n",
    "        a = None\n",
    "        new_s, r, done, truncated, info = None, None, None, None, None\n",
    "        assert new_s is not None and r is not None and done is not None\n",
    "        assert a is not None\n",
    "        # your code here\n",
    "        # Record state, action and add up reward to states,actions and total_reward accordingly.\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states, actions, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQ0YIlWu2QnH"
   },
   "outputs": [],
   "source": [
    "s, a, r = generate_session(env=env, policy=policy)\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) in [float, np.float64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "Ql2W5KVg2QnH",
    "outputId": "8dd8e1d5-47a8-4a75-e387-10ecd54fd461"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7c73eb3e2210>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAALz9JREFUeJzt3XlcVdW///H3YToMCjgyGAoaDpWzRZiNUti3+urNR6kPyyHTBqyUcqCHQ5pGUalXMzX7htY1+9a30m+T3i45/DIcMjVTMzUUh8AGAUEBhfX7w+u5ncD5IAt9PR+P/Yi919rrfM7Z4Hm39zr7OIwxRgAAABbxqu4CAAAA/oqAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwjk91F3A+ysvLdeDAAdWuXVsOh6O6ywEAAGfBGKPDhw8rMjJSXl6nP0dSIwPKgQMHFBUVVd1lAACA87B3715dccUVp+1TIwNK7dq1JZ14gsHBwdVcDQAAOBsFBQWKiopyvY+fTo0MKCcv6wQHBxNQAACoYc5megaTZAEAgHUIKAAAwDoEFAAAYJ0aOQcFAHB2ysrKdOzYseouA5cJb29v+fj4eOQWIAQUALhEFRYWat++fTLGVHcpuIwEBgYqIiJCfn5+FzQOAQUALkFlZWXat2+fAgMD1aBBA25qiSpnjFFpaal+/fVXZWVlKTY29ow3YzsdAgoAXIKOHTsmY4waNGiggICA6i4Hl4mAgAD5+vpqz549Ki0tlb+//3mPxSRZALiEceYEF9uFnDVxG8cjowAAAHgQAQUAcFmKjo7WtGnTqruMavfcc8+pXbt2rvUBAwaoR48e1VbPSeccUFauXKl77rlHkZGRcjgcWrRokVu7MUbjxo1TRESEAgIClJCQoB07drj1+eOPP9S3b18FBwcrNDRUgwYNUmFh4QU9EQBAzffcc8/J4XC4LS1btnTrU1xcrKSkJNWrV0+1atVSz549lZub62rfvXu326WtefPmKTQ09GI9BatV9r79zDPPKCMjo3oKOo1zDihFRUVq27atZs6cWWl7Wlqapk+frtmzZ2vNmjUKCgpSYmKiiouLXX369u2rLVu26Msvv9Snn36qlStXasiQIef/LAAAl4yrr75av/zyi2v5+uuv3dqHDx+uTz75RB988IFWrFihAwcO6N57762mai9cWVmZysvLq+3xa9WqpXr16lXb45+SuQCSzMcff+xaLy8vN+Hh4ebll192bcvLyzNOp9MsXLjQGGPM1q1bjSSzbt06V58vvvjCOBwOs3///rN63Pz8fCPJ5OfnX0j5AHDJOnr0qNm6das5evRodZdyTsaPH2/atm17yva8vDzj6+trPvjgA9e2bdu2GUkmMzPTGGNMVlaWOfn2tmzZMiPJbRk/frwxxpgmTZqYyZMnm4EDB5patWqZqKgoM2fOnNPWd/PNN5ukpCSTlJRkgoODTb169cyYMWNMeXm5q09xcbF5+umnTWRkpAkMDDTXXXedWbZsmas9PT3dhISEmMWLF5tWrVoZb29vk5WVZYqLi83IkSPNFVdcYfz8/EyzZs3Mm2++6dpv8+bNplu3biYoKMg0bNjQPPDAA+bXX391q+2JJ54wI0aMMHXq1DFhYWGu53ry+f75dWjSpEmlr3n//v1N9+7dXetlZWXmhRdeMNHR0cbf39+0adPG7fX/q9P97p3L+7dH56BkZWUpJydHCQkJrm0hISGKi4tTZmamJCkzM1OhoaHq1KmTq09CQoK8vLy0Zs2aSsctKSlRQUGB2wIAOHdFpUUXdTkfO3bsUGRkpJo2baq+ffsqOzvb1bZ+/XodO3bM7X2mZcuWaty4set95s86d+6sadOmKTg42HVG5plnnnG1v/rqq+rUqZM2bNigxx9/XI899pi2b99+2vrmz58vHx8frV27Vv/5n/+pKVOm6M0333S1Dx06VJmZmXrvvff0/fff67777lO3bt3cpjscOXJEL730kt58801t2bJFDRs2VL9+/bRw4UJNnz5d27Zt05w5c1SrVi1JUl5enm677Ta1b99e3377rZYsWaLc3Fzdf//9FWoLCgrSmjVrlJaWpokTJ+rLL7+UJK1bt06SlJ6erl9++cW1fiapqal6++23NXv2bG3ZskXDhw/XAw88oBUrVpzV/ufLo/dBycnJkSSFhYW5bQ8LC3O15eTkqGHDhu5F+Piobt26rj5/lZqaqgkTJniy1EteUWmRaqWe+MUuTClUkF9QNVcEwGOKiqT/feNSYaEUdPZ/3yf/XbhYzPhzu4ttXFyc5s2bpxYtWuiXX37RhAkTdOONN+qHH35Q7dq1lZOTIz8/vwpzSv78PhMdHe26e66fn59CQkLkcDgUHh5e4fH+9re/6fHHH5ckjRo1SlOnTtWyZcvUokWLU9YYFRWlqVOnyuFwqEWLFtq8ebOmTp2qwYMHKzs7W+np6crOzlZkZKSkE3M8lixZovT0dL3wwguSTtyn5vXXX1fbtm0lST/99JPef/99ffnll67w1bRpU9djvvbaa2rfvr1rf0l66623FBUVpZ9++knNmzeXJLVp00bjx4+XJMXGxuq1115TRkaGbr/9djVo0ECSFBoaWulrUZmSkhK98MIL+p//+R/Fx8e76vr66681Z84c3XzzzWc1zvmoETdqS0lJUXJysmu9oKBAUVFR1VgRAKAq3Hnnna6f27Rpo7i4ODVp0kTvv/++Bg0a5PHHa9OmjevnkyHm4MGDp93n+uuvd5uEGx8fr1dffVVlZWXavHmzysrKXIHhpJKSErd5Hn5+fm6PvXHjRnl7e5/yDX/Tpk1atmyZ64zKn+3atcstoPxZRETEGZ/P6ezcuVNHjhzR7bff7ra9tLRU7du3P+9xz4ZHA8rJRJabm6uIiAjX9tzcXNdHmCo7+MePH9cff/xxykTndDrldDo9WSoAXJYKU2rWJyZDQ0PVvHlz7dy5U9KJ95DS0lLl5eW5nUXJzc0967MCf+br6+u27nA4LmjCamFhoby9vbV+/Xp5e3u7tf05XAQEBLiFnDPd7bewsFD33HOPXnrppQptf36/rYrnI0mfffaZGjVq5NZW1e/LHg0oMTExCg8PV0ZGhiuQFBQUaM2aNXrssccknUiaeXl5Wr9+vTp27ChJ+uqrr1ReXq64uDhPlgMA+Iuadrm3sLBQu3bt0oMPPihJ6tixo3x9fZWRkaGePXtKkrZv367s7GzXJYi/8vPzU1lZmcdq+ut8ydWrVys2Nlbe3t5q3769ysrKdPDgQd14441nPWbr1q1VXl6uFStWuM2vOalDhw768MMPFR0dLR+f83/r9vX1PafX4qqrrpLT6VR2dnaVXs6pzDlPki0sLNTGjRu1ceNGSScmxm7cuFHZ2dlyOBwaNmyYJk2apH//+9/avHmz+vXrp8jISNdNX1q1aqVu3bpp8ODBWrt2rVatWqWhQ4eqd+/erut1AIDL0zPPPKMVK1Zo9+7d+uabb/Qf//Ef8vb2Vp8+fSSd+ODFoEGDlJycrGXLlmn9+vUaOHCg4uPjdf3111c6ZnR0tAoLC5WRkaHffvtNR44cuaAas7OzlZycrO3bt2vhwoWaMWOGnnrqKUlS8+bN1bdvX/Xr108fffSRsrKytHbtWqWmpuqzzz475ZjR0dHq37+/HnroIS1atEhZWVlavny53n//fUlSUlKS/vjjD/Xp00fr1q3Trl27tHTpUg0cOPCcAkd0dLQyMjKUk5OjQ4cOnbF/7dq19cwzz2j48OGaP3++du3ape+++04zZszQ/Pnzz/pxz8c5x7Bvv/1Wt956q2v95NyQ/v37a968eRo5cqSKioo0ZMgQ5eXlqUuXLlqyZInbFwYtWLBAQ4cOVdeuXeXl5aWePXtq+vTpHng6AICabN++ferTp49+//13NWjQQF26dNHq1atdEzwlaerUqa73jpKSEiUmJur1118/5ZidO3fWo48+ql69eun333/X+PHj9dxzz513jf369dPRo0d13XXXydvbW0899ZTbvbzS09M1adIkPf3009q/f7/q16+v66+/Xnffffdpx501a5aeffZZPf744/r999/VuHFjPfvss5KkyMhIrVq1SqNGjdIdd9yhkpISNWnSRN26dTun77559dVXlZycrLlz56pRo0bavXv3Gfd5/vnn1aBBA6Wmpurnn39WaGioOnTo4KqtqjjMyanONUhBQYFCQkKUn5+v4ODg6i7HSnyKB7iEncWneIqLi5WVlaWYmJgL+kZZuLvlllvUrl07bpF/Gqf73TuX92++iwcAAFiHgAIAAKxTI+6DAgCADZYvX17dJVw2OIMCAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoA4LJ0yy23aNiwYdVdRrWbN2+e2zdDP/fcc64v/K1OBBQAgDUOHz6sYcOGqUmTJgoICFDnzp21bt06tz7GGI0bN04REREKCAhQQkKCduzY4dbH4XC4vmdm+fLlcjgcysvLu0jPwl7R0dEVbtPfq1cv/fTTT9VT0GkQUAAA1nj44Yf15Zdf6p133tHmzZt1xx13KCEhQfv373f1SUtL0/Tp0zV79mytWbNGQUFBSkxMVHFxcTVWfv6MMTp+/Hi1PX5AQIAaNmxYbY9/KgQUAIAVjh49qg8//FBpaWm66aabdOWVV+q5557TlVdeqVmzZkk68WY+bdo0jRkzRt27d1ebNm309ttv68CBA1q0aFGFMXfv3q1bb71VklSnTh05HA4NGDDA1V5eXq6RI0eqbt26Cg8PP+O3HA8YMEA9evTQhAkT1KBBAwUHB+vRRx9VaWmp25ipqamKiYlRQECA2rZtq3/961+u9pNndL744gt17NhRTqdTX3/9tcrLy5WWlqYrr7xSTqdTjRs31uTJk1377d27V/fff79CQ0NVt25dde/e3e3biE/W9sorrygiIkL16tVTUlKSjh07JunEJa09e/Zo+PDhcjgccjgckipe4qnMm2++qVatWsnf318tW7Y87bdHewq3ugeAy0lR0cV9vEq+aflUjh8/rrKysgrfgBsQEKCvv/5akpSVlaWcnBwlJCS42kNCQhQXF6fMzEz17t3bbd+oqCh9+OGH6tmzp7Zv367g4GAFBAS42ufPn6/k5GStWbNGmZmZGjBggG644Qbdfvvtp6wzIyND/v7+Wr58uXbv3q2BAweqXr16rjCRmpqq//qv/9Ls2bMVGxurlStX6oEHHlCDBg108803u8YZPXq0XnnlFTVt2lR16tRRSkqK5s6dq6lTp6pLly765Zdf9OOPP0qSjh07psTERMXHx+v//b//Jx8fH02aNEndunXT999/Lz8/P0nSsmXLFBERoWXLlmnnzp3q1auX2rVrp8GDB+ujjz5S27ZtNWTIEA0ePPisj8uCBQs0btw4vfbaa2rfvr02bNigwYMHKygoSP379z/rcc6ZqYHy8/ONJJOfn1/dpVirsKTQ6DkZPSdTWFJY3eUA8KTCQmOkE0th5X/fR48eNVu3bjVHjx51bzi538VazlF8fLy5+eabzf79+83x48fNO++8Y7y8vEzz5s2NMcasWrXKSDIHDhxw2+++++4z999/f6VjLlu2zEgyhw4dctt+8803my5durhtu/baa82oUaNOWV///v1N3bp1TVFRkWvbrFmzTK1atUxZWZkpLi42gYGB5ptvvnHbb9CgQaZPnz5u9SxatMjVXlBQYJxOp5k7d26lj/vOO++YFi1amPLycte2kpISExAQYJYuXeqqrUmTJub48eOuPvfdd5/p1auXa71JkyZm6tSpbmOnp6ebkJAQ1/r48eNN27ZtXevNmjUz7777rts+zz//vImPj6+01lP+7plze//mDAoAwBrvvPOOHnroITVq1Eje3t7q0KGD+vTpo/Xr11fJ47Vp08ZtPSIiQgcPHjztPm3btlVgYKBrPT4+XoWFhdq7d68KCwt15MiRCmdgSktL1b59e7dtnTp1cv28bds2lZSUqGvXrpU+5qZNm7Rz507Vrl3bbXtxcbF27drlWr/66qvl7e3t9nw2b9582udzOkVFRdq1a5cGDRrkdtbl+PHjCgkJOe9xzwYBBQAuJ4WF1V3BaTVr1kwrVqxQUVGRCgoKFBERoV69eqlp06aSpPDwcElSbm6uIiIiXPvl5uae10djfX193dYdDofKy8vPu/7C/319P/vsMzVq1Mitzel0uq0H/eny158vO51q3I4dO2rBggUV2ho0aOD6uaqez9y5cxUXF+fW9ucgVBUIKABwOTmHOSHVKSgoSEFBQTp06JCWLl2qtLQ0SVJMTIzCw8OVkZHhCiQFBQVas2aNHnvssUrHOjk/o6yszCO1bdq0SUePHnWFitWrV6tWrVqKiopS3bp15XQ6lZ2d7Tbf5ExiY2MVEBCgjIwMPfzwwxXaO3TooH/+859q2LChgoODz7t2Pz+/c3odwsLCFBkZqZ9//ll9+/Y978c9HwQUAIA1li5dKmOMWrRooZ07d2rEiBFq2bKlBg4cKOnEGYFhw4Zp0qRJio2NVUxMjMaOHavIyEj16NGj0jGbNGkih8OhTz/9VH/7298UEBCgWrVqnXeNpaWlGjRokMaMGaPdu3dr/PjxGjp0qLy8vFS7dm0988wzGj58uMrLy9WlSxfl5+dr1apVCg4OPuWkUn9/f40aNUojR46Un5+fbrjhBv3666/asmWLBg0apL59++rll19W9+7dNXHiRF1xxRXas2ePPvroI40cOVJXXHHFWdUeHR2tlStXqnfv3nI6napfv/4Z95kwYYKefPJJhYSEqFu3biopKdG3336rQ4cOKTk5+Zxeu3NBQAEAWCM/P18pKSnat2+f6tatq549e2ry5Mluly5GjhypoqIiDRkyRHl5eerSpYuWLFlS4dM/JzVq1EgTJkzQ6NGjNXDgQPXr10/z5s077xq7du2q2NhY3XTTTSopKVGfPn3cPp78/PPPq0GDBkpNTdXPP/+s0NBQdejQQc8+++xpxx07dqx8fHw0btw4HThwQBEREXr00UclSYGBgVq5cqVGjRqle++9V4cPH1ajRo3UtWvXczqjMnHiRD3yyCNq1qyZSkpKZIw54z4PP/ywAgMD9fLLL2vEiBEKCgpS69atq/wuvA5zNtVZpqCgQCEhIcrPz7+gU12XsqLSItVKPfF/CIUphQryqxmndQGchaIi6eQZgMLCSi/bFBcXKysrSzExMad848a5GzBggPLy8iq95wpOON3v3rm8f3OjNgAAYB0CCgAAsA5zUAAAOEsXMncF54YzKAAAwDoEFAAAYB0CCgBcwmrgBzVRw3nqd46AAgCXoJO3IS8tLa3mSnC5OXLkiKSKt90/V0ySBYBLkI+PjwIDA/Xrr7/K19dXXl78/yiqljFGR44c0cGDBxUaGnrB39VDQAGAS5DD4VBERISysrK0Z8+e6i4Hl5HQ0FDXlzpeCAIKAFyi/Pz8FBsby2UeXDS+vr4e+5ZjAgoAXMK8vLy41T1qJC5KAgAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOh4PKGVlZRo7dqxiYmIUEBCgZs2a6fnnn5cxxtXHGKNx48YpIiJCAQEBSkhI0I4dOzxdCgAAqKE8HlBeeuklzZo1S6+99pq2bduml156SWlpaZoxY4arT1pamqZPn67Zs2drzZo1CgoKUmJiooqLiz1dDgAAqIF8PD3gN998o+7du+uuu+6SJEVHR2vhwoVau3atpBNnT6ZNm6YxY8aoe/fukqS3335bYWFhWrRokXr37u3pkgAAQA3j8TMonTt3VkZGhn766SdJ0qZNm/T111/rzjvvlCRlZWUpJydHCQkJrn1CQkIUFxenzMxMT5cDAABqII+fQRk9erQKCgrUsmVLeXt7q6ysTJMnT1bfvn0lSTk5OZKksLAwt/3CwsJcbX9VUlKikpIS13pBQYGnywYAABbx+BmU999/XwsWLNC7776r7777TvPnz9crr7yi+fPnn/eYqampCgkJcS1RUVEerBgAANjG4wFlxIgRGj16tHr37q3WrVvrwQcf1PDhw5WamipJCg8PlyTl5ua67Zebm+tq+6uUlBTl5+e7lr1793q6bAAAYBGPB5QjR47Iy8t9WG9vb5WXl0uSYmJiFB4eroyMDFd7QUGB1qxZo/j4+ErHdDqdCg4OdlsAAMCly+NzUO655x5NnjxZjRs31tVXX60NGzZoypQpeuihhyRJDodDw4YN06RJkxQbG6uYmBiNHTtWkZGR6tGjh6fLAQAANZDHA8qMGTM0duxYPf744zp48KAiIyP1yCOPaNy4ca4+I0eOVFFRkYYMGaK8vDx16dJFS5Yskb+/v6fLAQAANZDD/PkWrzVEQUGBQkJClJ+fz+WeUygqLVKt1FqSpMKUQgX5BVVzRQA8pqhIqnXi71uFhVIQf9+oGc7l/Zvv4gEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALCOT3UXcDmJHv1ZlY29+8W7qmxsAAAuNs6gAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALBOlQSU/fv364EHHlC9evUUEBCg1q1b69tvv3W1G2M0btw4RUREKCAgQAkJCdqxY0dVlAIAAGogjweUQ4cO6YYbbpCvr6+++OILbd26Va+++qrq1Knj6pOWlqbp06dr9uzZWrNmjYKCgpSYmKji4mJPlwMAAGogH08P+NJLLykqKkrp6emubTExMa6fjTGaNm2axowZo+7du0uS3n77bYWFhWnRokXq3bu3p0sCAAA1jMfPoPz73/9Wp06ddN9996lhw4Zq37695s6d62rPyspSTk6OEhISXNtCQkIUFxenzMzMSscsKSlRQUGB2wIAAC5dHg8oP//8s2bNmqXY2FgtXbpUjz32mJ588knNnz9fkpSTkyNJCgsLc9svLCzM1fZXqampCgkJcS1RUVGeLhsAAFjE4wGlvLxcHTp00AsvvKD27dtryJAhGjx4sGbPnn3eY6akpCg/P9+17N2714MVAwAA23g8oEREROiqq65y29aqVStlZ2dLksLDwyVJubm5bn1yc3NdbX/ldDoVHBzstgAAgEuXxwPKDTfcoO3bt7tt++mnn9SkSRNJJybMhoeHKyMjw9VeUFCgNWvWKD4+3tPlAACAGsjjn+IZPny4OnfurBdeeEH333+/1q5dqzfeeENvvPGGJMnhcGjYsGGaNGmSYmNjFRMTo7FjxyoyMlI9evTwdDkAAKAG8nhAufbaa/Xxxx8rJSVFEydOVExMjKZNm6a+ffu6+owcOVJFRUUaMmSI8vLy1KVLFy1ZskT+/v6eLgcAANRAHg8oknT33Xfr7rvvPmW7w+HQxIkTNXHixKp4eAAAUMPxXTwAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6/hUdwHwjOjRn7mtl6tYCjjxc6txS+Ql//Mad/eLd11oaQAAnDPOoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6VR5QXnzxRTkcDg0bNsy1rbi4WElJSapXr55q1aqlnj17Kjc3t6pLAQAANUSVBpR169Zpzpw5atOmjdv24cOH65NPPtEHH3ygFStW6MCBA7r33nurshQAAFCDVFlAKSwsVN++fTV37lzVqVPHtT0/P1//+Mc/NGXKFN12223q2LGj0tPT9c0332j16tVVVQ4AAKhBqiygJCUl6a677lJCQoLb9vXr1+vYsWNu21u2bKnGjRsrMzOz0rFKSkpUUFDgtgAAgEuXT1UM+t577+m7777TunXrKrTl5OTIz89PoaGhbtvDwsKUk5NT6XipqamaMGFCVZQKAAAs5PEzKHv37tVTTz2lBQsWyN/f3yNjpqSkKD8/37Xs3bvXI+MCAAA7eTygrF+/XgcPHlSHDh3k4+MjHx8frVixQtOnT5ePj4/CwsJUWlqqvLw8t/1yc3MVHh5e6ZhOp1PBwcFuCwAAuHR5/BJP165dtXnzZrdtAwcOVMuWLTVq1ChFRUXJ19dXGRkZ6tmzpyRp+/btys7OVnx8vKfLAQAANZDHA0rt2rV1zTXXuG0LCgpSvXr1XNsHDRqk5ORk1a1bV8HBwXriiScUHx+v66+/3tPlAACAGqhKJsmeydSpU+Xl5aWePXuqpKREiYmJev3116ujFAAAYKGLElCWL1/utu7v76+ZM2dq5syZF+PhAQBADcN38QAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHY8HlNTUVF177bWqXbu2GjZsqB49emj79u1ufYqLi5WUlKR69eqpVq1a6tmzp3Jzcz1dCgAAqKE8HlBWrFihpKQkrV69Wl9++aWOHTumO+64Q0VFRa4+w4cP1yeffKIPPvhAK1as0IEDB3Tvvfd6uhQAAFBD+Xh6wCVLlritz5s3Tw0bNtT69et10003KT8/X//4xz/07rvv6rbbbpMkpaenq1WrVlq9erWuv/56T5cEAABqmCqfg5Kfny9Jqlu3riRp/fr1OnbsmBISElx9WrZsqcaNGyszM7PSMUpKSlRQUOC2AACAS1eVBpTy8nINGzZMN9xwg6655hpJUk5Ojvz8/BQaGurWNywsTDk5OZWOk5qaqpCQENcSFRVVlWUDAIBqVqUBJSkpST/88IPee++9CxonJSVF+fn5rmXv3r0eqhAAANjI43NQTho6dKg+/fRTrVy5UldccYVre3h4uEpLS5WXl+d2FiU3N1fh4eGVjuV0OuV0OquqVAAAYBmPn0Exxmjo0KH6+OOP9dVXXykmJsatvWPHjvL19VVGRoZr2/bt25Wdna34+HhPlwMAAGogj59BSUpK0rvvvqvFixerdu3arnklISEhCggIUEhIiAYNGqTk5GTVrVtXwcHBeuKJJxQfH88neAAAgKQqCCizZs2SJN1yyy1u29PT0zVgwABJ0tSpU+Xl5aWePXuqpKREiYmJev311z1dCgAAqKE8HlCMMWfs4+/vr5kzZ2rmzJmefngAAHAJ4Lt4AACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWMenuguwUfToz6q7BAAALmucQQEAANYhoAAAAOsQUAAAgHWYg4LTqsr5OLtfvKvKxgYA1GycQQEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1uFOsgBgub/e0TmgtFjb/vfnVmOX6Kif/3mNy92cYTPOoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdbjVParNX2/f7SlVefvuqqq5puJW6QCqCmdQAACAdQgoAADAOlziAXBZqYmXFlGzVeWl4Uv5944zKAAAwDoEFAAAYB0CCgAAsA5zUHDJ4aPAFw+v9f/htbg4mM/h7lJ+PTiDAgAArENAAQAA1uESDwDA47jkhQvFGRQAAGAdAgoAALBOtQaUmTNnKjo6Wv7+/oqLi9PatWursxwAAGCJapuD8s9//lPJycmaPXu24uLiNG3aNCUmJmr79u1q2LBhdZUFAJcN5om44/WwS7WdQZkyZYoGDx6sgQMH6qqrrtLs2bMVGBiot956q7pKAgAAlqiWMyilpaVav369UlJSXNu8vLyUkJCgzMzMCv1LSkpUUlLiWs/Pz5ckFRQUVEl95SVHqmTci6lcxZLjf38uOSKpvFrrAeA5ZaXFOvmvX1nJEZUb/r7heVXxHntyTGPMGftWS0D57bffVFZWprCwMLftYWFh+vHHHyv0T01N1YQJEypsj4qKqrIaLyX71a+6SwDgYSEnf3idv29UjZBpVTf24cOHFRIScto+NeI+KCkpKUpOTnatl5eX648//lC9evXkcDiqsbKqUVBQoKioKO3du1fBwcHVXQ7OgONVc3Csag6OVc1ytsfLGKPDhw8rMjLyjGNWS0CpX7++vL29lZub67Y9NzdX4eHhFfo7nU45nU63baGhoVVZohWCg4P5w6xBOF41B8eq5uBY1Sxnc7zOdObkpGqZJOvn56eOHTsqIyPDta28vFwZGRmKj4+vjpIAAIBFqu0ST3Jysvr3769OnTrpuuuu07Rp01RUVKSBAwdWV0kAAMAS1RZQevXqpV9//VXjxo1TTk6O2rVrpyVLllSYOHs5cjqdGj9+fIXLWrATx6vm4FjVHByrmqUqjpfDnM1nfQAAAC4ivosHAABYh4ACAACsQ0ABAADWIaAAAADrEFCqQUlJidq1ayeHw6GNGze6tX3//fe68cYb5e/vr6ioKKWlpVXY/4MPPlDLli3l7++v1q1b6/PPP3drN8Zo3LhxioiIUEBAgBISErRjx46qfEqXpL///e9q3Lix/P39FRERoQcffFAHDhxw68Pxqn67d+/WoEGDFBMTo4CAADVr1kzjx49XaWmpWz+OlR0mT56szp07KzAw8JQ33MzOztZdd92lwMBANWzYUCNGjNDx48fd+ixfvlwdOnSQ0+nUlVdeqXnz5lUYZ+bMmYqOjpa/v7/i4uK0du3aKnhGqLLX2eCie/LJJ82dd95pJJkNGza4tufn55uwsDDTt29f88MPP5iFCxeagIAAM2fOHFefVatWGW9vb5OWlma2bt1qxowZY3x9fc3mzZtdfV588UUTEhJiFi1aZDZt2mT+/ve/m5iYGHP06NGL+TRrvClTppjMzEyze/dus2rVKhMfH2/i4+Nd7RwvO3zxxRdmwIABZunSpWbXrl1m8eLFpmHDhubpp5929eFY2WPcuHFmypQpJjk52YSEhFRoP378uLnmmmtMQkKC2bBhg/n8889N/fr1TUpKiqvPzz//bAIDA01ycrLZunWrmTFjhvH29jZLlixx9XnvvfeMn5+feeutt8yWLVvM4MGDTWhoqMnNzb0YT/OyUZWvMwHlIvv8889Ny5YtzZYtWyoElNdff93UqVPHlJSUuLaNGjXKtGjRwrV+//33m7vuusttzLi4OPPII48YY4wpLy834eHh5uWXX3a15+XlGafTaRYuXFhFz+rysHjxYuNwOExpaakxhuNls7S0NBMTE+Na51jZJz09vdKA8vnnnxsvLy+Tk5Pj2jZr1iwTHBzsOn4jR440V199tdt+vXr1MomJia716667ziQlJbnWy8rKTGRkpElNTfXwM7m8VeXrzCWeiyg3N1eDBw/WO++8o8DAwArtmZmZuummm+Tn5+falpiYqO3bt+vQoUOuPgkJCW77JSYmKjMzU5KUlZWlnJwctz4hISGKi4tz9cG5++OPP7RgwQJ17txZvr6+kjheNsvPz1fdunVd6xyrmiMzM1OtW7d2u2lnYmKiCgoKtGXLFlef0x2r0tJSrV+/3q2Pl5eXEhISOFYeVNWvMwHlIjHGaMCAAXr00UfVqVOnSvvk5ORUuJPuyfWcnJzT9vlz+5/3q6wPzt6oUaMUFBSkevXqKTs7W4sXL3a1cbzstHPnTs2YMUOPPPKIaxvHqua4kGNVUFCgo0eP6rffflNZWRnHqopV9etMQLlAo0ePlsPhOO3y448/asaMGTp8+LBSUlKqu+TL2tker5NGjBihDRs26L//+7/l7e2tfv36yXDz5YviXI+VJO3fv1/dunXTfffdp8GDB1dT5Zef8zlWwJlU23fxXCqefvppDRgw4LR9mjZtqq+++kqZmZkVvqegU6dO6tu3r+bPn6/w8HDl5ua6tZ9cDw8Pd/23sj5/bj+5LSIiwq1Pu3btzvn5XWrO9nidVL9+fdWvX1/NmzdXq1atFBUVpdWrVys+Pp7jVcXO9VgdOHBAt956qzp37qw33njDrR/Hqmqd67E6nfDw8AqfAjnbYxUcHKyAgAB5e3vL29v7tMcTF65+/fpV+zpf8CwWnJU9e/aYzZs3u5alS5caSeZf//qX2bt3rzHm/ybynZyEaYwxKSkpFSby3X333W5jx8fHV5jI98orr7ja8/PzmcjnAXv27DGSzLJly4wxHC+b7Nu3z8TGxprevXub48ePV2jnWNnnTJNk//wpkDlz5pjg4GBTXFxsjDkxSfaaa65x269Pnz4VJskOHTrUtV5WVmYaNWrEJFkPq8rXmYBSTbKysip8iicvL8+EhYWZBx980Pzwww/mvffeM4GBgRU+Cunj42NeeeUVs23bNjN+/PhKPwoZGhpqFi9ebL7//nvTvXt3Pgp5jlavXm1mzJhhNmzYYHbv3m0yMjJM586dTbNmzVz/SHK87LBv3z5z5ZVXmq5du5p9+/aZX375xbWcxLGyx549e8yGDRvMhAkTTK1atcyGDRvMhg0bzOHDh40x//cx4zvuuMNs3LjRLFmyxDRo0KDSjxmPGDHCbNu2zcycObPSjxk7nU4zb948s3XrVjNkyBATGhrq9ukgXLiqfJ0JKNWksoBijDGbNm0yXbp0MU6n0zRq1Mi8+OKLFfZ9//33TfPmzY2fn5+5+uqrzWeffebWXl5ebsaOHWvCwsKM0+k0Xbt2Ndu3b6/Kp3PJ+f77782tt95q6tata5xOp4mOjjaPPvqo2bdvn1s/jlf1S09PN5IqXf6MY2WH/v37V3qsTp6ZNMaY3bt3mzvvvNMEBASY+vXrm6efftocO3bMbZxly5aZdu3aGT8/P9O0aVOTnp5e4bFmzJhhGjdubPz8/Mx1111nVq9eXcXP7vJUVa+zwxhm/AEAALvwKR4AAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArPP/AeKgJSPM3K6ZAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's see the initial reward distribution\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sample_rewards = [generate_session(env=env, policy=policy, t_max=1000)[-1] for _ in range(200)]\n",
    "\n",
    "plt.hist(sample_rewards, bins=20)\n",
    "plt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color='green')\n",
    "plt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4FHOyN_X2QnH",
    "outputId": "fc3b3341-795e-4d45-f1ad-6fc861b3c9c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-3835.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(sample_rewards, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Kj4CEgl2QnI"
   },
   "source": [
    "### Crossentropy method steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BnDggvxU2QnI"
   },
   "outputs": [],
   "source": [
    "def select_elites(states_batch, actions_batch, rewards_batch, percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n",
    "\n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "\n",
    "    Please return elite states and actions in their original order\n",
    "    [i.e. sorted by session number and timestep within session]\n",
    "\n",
    "    If you are confused, see examples below. Please don't assume that states are integers\n",
    "    (they will become different later).\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cx3oUy-E2QnI"
   },
   "outputs": [],
   "source": [
    "states_batch = [\n",
    "    [1, 2, 3],     # game1\n",
    "    [4, 2, 0, 2],  # game2\n",
    "    [3, 1],        # game3\n",
    "]\n",
    "\n",
    "actions_batch = [\n",
    "    [0, 2, 4],     # game1\n",
    "    [3, 2, 0, 1],  # game2\n",
    "    [3, 3],        # game3\n",
    "]\n",
    "rewards_batch = [\n",
    "    3,  # game1\n",
    "    4,  # game2\n",
    "    5,  # game3\n",
    "]\n",
    "\n",
    "test_result_0 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=0)\n",
    "test_result_40 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=30)\n",
    "test_result_90 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=90)\n",
    "test_result_100 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=100)\n",
    "\n",
    "assert np.all(test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1])  \\\n",
    "    and np.all(test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]),\\\n",
    "    \"For percentile 0 you should return all states and actions in chronological order\"\n",
    "assert np.all(test_result_40[0] == [4, 2, 0, 2, 3, 1]) and \\\n",
    "    np.all(test_result_40[1] == [3, 2, 0, 1, 3, 3]),\\\n",
    "    \"For percentile 30 you should only select states/actions from two first\"\n",
    "assert np.all(test_result_90[0] == [3, 1]) and \\\n",
    "    np.all(test_result_90[1] == [3, 3]),\\\n",
    "    \"For percentile 90 you should only select states/actions from one game\"\n",
    "assert np.all(test_result_100[0] == [3, 1]) and\\\n",
    "    np.all(test_result_100[1] == [3, 3]),\\\n",
    "    \"Please make sure you use >=, not >. Also double-check how you compute percentile.\"\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrbmNSOw2QnI"
   },
   "outputs": [],
   "source": [
    "def update_policy(elite_states, elite_actions, n_states=n_states, n_actions=n_actions):\n",
    "    \"\"\"\n",
    "    Given old policy and a list of elite states/actions from select_elites,\n",
    "    return new updated policy where each action probability is proportional to\n",
    "\n",
    "    policy[s_i,a_i] ~ #[occurences of si and ai in elite states/actions]\n",
    "\n",
    "    Don't forget to normalize policy to get valid probabilities and handle 0/0 case.\n",
    "    In case you never visited a state, set probabilities for all actions to 1./n_actions\n",
    "\n",
    "    :param elite_states: 1D list of states from elite sessions\n",
    "    :param elite_actions: 1D list of actions from elite sessions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4Hcnu5R2QnI"
   },
   "outputs": [],
   "source": [
    "elite_states = [1, 2, 3, 4, 2, 0, 2, 3, 1]\n",
    "elite_actions = [0, 2, 4, 3, 2, 0, 1, 3, 3]\n",
    "\n",
    "new_policy = update_policy(elite_states, elite_actions)\n",
    "\n",
    "assert np.isfinite(new_policy).all(\n",
    "), \"Your new policy contains NaNs or +-inf. Make sure you don't divide by zero.\"\n",
    "assert np.all(\n",
    "    new_policy >= 0), \"Your new policy can't have negative action probabilities\"\n",
    "assert np.allclose(new_policy.sum(\n",
    "    axis=-1), 1), \"Your new policy should be a valid probability distribution over actions\"\n",
    "reference_answer = np.array([\n",
    "    [1.,  0.,  0.,  0.,  0.],\n",
    "    [0.5,  0.,  0.,  0.5,  0.],\n",
    "    [0.,  0.33333333,  0.66666667,  0.,  0.],\n",
    "    [0.,  0.,  0.,  0.5,  0.5]])\n",
    "assert np.allclose(new_policy[:4, :5], reference_answer)\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c04c-xlK2QnI"
   },
   "source": [
    "### Training loop\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-AS7Kgl2QnI"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def show_progress(rewards_batch, log, percentile, reward_range=[-990, +10]):\n",
    "    \"\"\"\n",
    "    A convenience function that displays training progress.\n",
    "    No cool math here, just charts.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    threshold = np.percentile(rewards_batch, percentile)\n",
    "    log.append([mean_reward, threshold])\n",
    "\n",
    "    clear_output(True)\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\" % (mean_reward, threshold))\n",
    "    plt.figure(figsize=[8, 4])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
    "    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(rewards_batch, range=reward_range)\n",
    "    plt.vlines([np.percentile(rewards_batch, percentile)],\n",
    "               [0], [100], label=\"percentile\", color='red')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Jc9G4Fq2QnJ"
   },
   "outputs": [],
   "source": [
    "# reset policy just in case\n",
    "policy = np.ones([n_states, n_actions]) / n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4JXholL2QnJ"
   },
   "outputs": [],
   "source": [
    "n_sessions = 250  # sample this many sessions\n",
    "percentile = 50  # take this percent of session with highest rewards\n",
    "learning_rate = 0.5  # add this thing to all counts for stability\n",
    "\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    %time sessions = [generate_session(policy) for _ in range(n_sessions)]\n",
    "\n",
    "    states_batch, actions_batch, rewards_batch = zip(*sessions)\n",
    "\n",
    "    elite_states, elite_actions = select_elites(states_batch, actions_batch, rewards_batch, percentile)\n",
    "\n",
    "    new_policy = update_policy(elite_states, elite_actions)\n",
    "\n",
    "    policy = learning_rate*new_policy + (1-learning_rate)*policy\n",
    "\n",
    "    # display results on chart\n",
    "    show_progress(rewards_batch, log, percentile)\n",
    "    if np.mean(rewards_batch) > -50:\n",
    "        print(\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've just trained your first RL model!\n",
    "Now copy functions from this notebook to `template_crossentropy.py` and submit it to the Contest problem `Tabular CrossEntropy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3Y2oAQy2QnJ"
   },
   "source": [
    "## Part 2: Approximate crossentropy and neural nets.\n",
    "\n",
    "In this section you will train a neural network policy for continuous state space game\n",
    "\n",
    "You can find full description of the environment [here](https://www.gymlibrary.dev/environments/classic_control/cart_pole/).\n",
    "\n",
    "![CartPole-v0\"](https://www.gymlibrary.dev/_images/cart_pole.gif)\n",
    "\n",
    "So here's how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsYsd8mt2QnJ"
   },
   "outputs": [],
   "source": [
    "# if you see \"<classname> has no attribute .env\", remove .env or update gym\n",
    "env = gym.make(\"CartPole-v0\", render_mode=\"rgb_array\").env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-ocju_G2QnJ"
   },
   "outputs": [],
   "source": [
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is fine to use sklearn for such simple network, but we recommend you write the solution with PyTorch ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwizLaQA2QnJ"
   },
   "outputs": [],
   "source": [
    "# create agent\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# YOUR CODE GOES HERE: implement your own agent\n",
    "agent = ...\n",
    "\n",
    "# initialize agent to the dimension of state an amount of actions\n",
    "agent.fit([env.reset()[0]]*n_actions, range(n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the generate_session function and beat the game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vhFrvKI2QnJ"
   },
   "outputs": [],
   "source": [
    "def generate_session_cartpole(t_max=300):\n",
    "\n",
    "    states, actions = [], []\n",
    "    total_reward = 0\n",
    "\n",
    "    s, info = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # YOUR CODE GOES HERE: choose action!\n",
    "        raise NotImplementedError\n",
    "\n",
    "        new_s, r, done, truncated, info = env.step(a)\n",
    "\n",
    "        # record sessions like you did before\n",
    "\n",
    "        states.append(s.astype(dtype=\"object\"))\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states, actions, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KKFnnrf2QnK"
   },
   "outputs": [],
   "source": [
    "n_sessions = 100\n",
    "percentile = 70\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "    # generate new sessions\n",
    "    sessions = [generate_session_cartpole() for _ in range(n_sessions)]\n",
    "\n",
    "    states_batch, actions_batch, rewards_batch = zip(*sessions)\n",
    "\n",
    "    elite_states, elite_actions = select_elites(states_batch, actions_batch, rewards_batch, percentile)\n",
    "\n",
    "    agent.fit(elite_states, elite_actions)\n",
    "\n",
    "    if max(rewards_batch) > min(rewards_batch):\n",
    "        show_progress(rewards_batch, log, percentile, reward_range=[0, np.max(rewards_batch)])\n",
    "\n",
    "    if np.mean(rewards_batch) > 210:\n",
    "        print(\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission generation\n",
    "To generate submission run the following block of code and submit the generated file to the contest problem `CrossEntropy CartPole`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS BLOCK\n",
    "sessions = [generate_session_cartpole() for _ in range(n_sessions)]\n",
    "states_batch, actions_batch, rewards_batch = zip(*sessions)\n",
    "elite_states, elite_actions = select_elites(states_batch, actions_batch, rewards_batch, percentile)\n",
    "\n",
    "sessions_to_send = []\n",
    "for session in sessions:\n",
    "  observations = [x.tolist() for x in session[0]]\n",
    "  actions = [x.item() for x in session[1]]\n",
    "  sessions_to_send.append((observations, actions))\n",
    "\n",
    "import json\n",
    "with open('sessions_to_send.json', 'w') as iofile:\n",
    "  json.dump(sessions_to_send, iofile, ensure_ascii=True, indent=4)\n",
    "# DO NOT CHANGE THIS BLOCK"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
